{
 "metadata": {
  "name": "intro_nlp_blog_post_pt2"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "\n",
      "## Intro to Natural Language Processing \u2014 Part 2\n",
      "\n",
      "---\n",
      "\n",
      "Code written for this blog post: http://datasciencerules.blogspot.com/2013/04/intro-to-natural-language-processing.html\n",
      "\n",
      "A single Thomas Friedman NYT column is used as a case study for basic NLP analysis."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import bs4\n",
      "import nltk\n",
      "import requests"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "response = requests.get('http://www.nytimes.com/2013/04/07/opinion/sunday/friedman-weve-wasted-our-timeout.html')\n",
      "soup = bs4.BeautifulSoup(response.text)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "article = ''\n",
      "paragraphs = soup.find_all('p', itemprop='articleBody')\n",
      "for paragraph in paragraphs:\n",
      "    article += paragraph.get_text()\n",
      "article.encode('ascii', errors='replace')\n",
      "print article[:500]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "YES, it\u2019s true \u2014 a crisis is a terrible thing to waste. But a \u201ctimeout\u201d is also a terrible thing to waste, and as I look at the world today I wonder if that\u2019s exactly what we\u2019ve just done. We\u2019ve wasted a five-year timeout from geopolitics, and if we don\u2019t wake up and get our act together as a country \u2014 and if the Chinese, Russians and Europeans don\u2019t do the same \u2014 we\u2019re all really going to regret it.        \n",
        "Think about what a relative luxury we\u2019ve enjoyed since the Great Recession hit in 2008.\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def clean_text(text):\n",
      "    \n",
      "    from nltk import clean_html\n",
      "    import re\n",
      "    \n",
      "    # strip html markup with handy NLTK function\n",
      "    text = clean_html(text)        \n",
      "    # remove digits with regular expression\n",
      "    text = re.sub(r'\\d', ' ', text)\n",
      "    # remove any patterns matching standard url format\n",
      "    url_pattern = r'((http|ftp|https):\\/\\/)?[\\w\\-_]+(\\.[\\w\\-_]+)+([\\w\\-\\.,@?^=%&amp;:/~\\+#]*[\\w\\-\\@?^=%&amp;/~\\+#])?'\n",
      "    text = re.sub(url_pattern, ' ', text)\n",
      "    # remove all non-ascii characters\n",
      "    text = ''.join(character for character in text if ord(character)<128)\n",
      "    # standardize white space\n",
      "    text = re.sub(r'\\s+', ' ', text)\n",
      "    # drop capitalization\n",
      "    text = text.lower()\n",
      "    \n",
      "    return text"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clean_article = clean_text(article)\n",
      "print clean_article[:500]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "yes, its true a crisis is a terrible thing to waste. but a timeout is also a terrible thing to waste, and as i look at the world today i wonder if thats exactly what weve just done. weve wasted a five-year timeout from geopolitics, and if we dont wake up and get our act together as a country and if the chinese, russians and europeans dont do the same were all really going to regret it. think about what a relative luxury weve enjoyed since the great recession hit in . we, the europeans and the wo\n"
       ]
      }
     ],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def tokenize_and_normalize_doc(doc, filter_stopwords=True, normalize='lemma'):\n",
      "    \n",
      "    import nltk.corpus\n",
      "    from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
      "    from nltk.tokenize import sent_tokenize, word_tokenize, wordpunct_tokenize\n",
      "    from string import punctuation\n",
      "    \n",
      "    # use NLTK's default set of english stop words\n",
      "    stops_list = nltk.corpus.stopwords.words('english')\n",
      "    \n",
      "    if normalize == 'lemma':\n",
      "        # lemmatize with WordNet\n",
      "        normalizer = WordNetLemmatizer()\n",
      "    elif normalize == 'stem':\n",
      "        # stem with Porter\n",
      "        normalizer = PorterStemmer()\n",
      "    \n",
      "    # tokenize the document into sentences with NLTK default\n",
      "    sents = sent_tokenize(doc)\n",
      "    # tokenize each sentence into words with NLTK default\n",
      "    #tokenized_sents = [wordpunct_tokenize(sent) for sent in sents]\n",
      "    tokenized_sents = [word_tokenize(sent) for sent in sents]\n",
      "    # filter out \"bad\" words, normalize good ones\n",
      "    normalized_sents = []\n",
      "    for tokenized_sent in tokenized_sents:\n",
      "        good_words = [word for word in tokenized_sent\n",
      "                      # filter out too-long words\n",
      "                      if len(word) < 25\n",
      "                      # filter out punctuation\n",
      "                      if word not in list(punctuation)]\n",
      "        if filter_stopwords is True:\n",
      "            good_words = [word for word in good_words\n",
      "                          # filter out stop words\n",
      "                          if word not in stops_list]\n",
      "        if normalize == 'lemma':\n",
      "            normalized_sents.append([normalizer.lemmatize(word) for word in good_words])\n",
      "        elif normalize == 'stem':\n",
      "            normalized_sents.append([normalizer.stem(word) for word in good_words])\n",
      "        else:\n",
      "            normalized_sents.append([word for word in good_words])\n",
      "    \n",
      "    return normalized_sents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokenized_normalized_article = tokenize_and_normalize_doc(clean_article)\n",
      "for sent in tokenized_normalized_article[:10]:\n",
      "    print sent"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'yes', u'true', u'crisis', u'terrible', u'thing', u'waste']\n",
        "[u'timeout', u'also', u'terrible', u'thing', u'waste', u'look', u'world', u'today', u'wonder', u'thats', u'exactly', u'weve', u'done']\n",
        "[u'weve', u'wasted', u'five-year', u'timeout', u'geopolitics', u'dont', u'wake', u'get', u'act', u'together', u'country', u'chinese', u'russian', u'european', u'dont', u'really', u'going', u'regret']\n",
        "[u'think', u'relative', u'luxury', u'weve', u'enjoyed', u'since', u'great', u'recession', u'hit']\n",
        "[u'european', u'world', u'major', u'power', u'able', u'focus', u'almost', u'entirely', u'healing', u'economy', u'without', u'worry', u'major', u'war', u'globe-rattling', u'conflict', u'would', u'snuff', u'fragile', u'economic', u'recovery', u'require', u'extensive', u'new', u'defense', u'spending']\n",
        "[u'relatively', u'speaking', u'world', u'last', u'five', u'year', u'geopolitical', u'timeout']\n",
        "[u'everywhere', u'turn', u'see', u'different', u'actor', u'standing', u'toe', u'red', u'line', u'seemingly', u'ready', u'willing', u'even', u'itching', u'cross', u'moment']\n",
        "[u'north', u'korea', u'boy', u'king', u'kim', u'jong-un', u'seems', u'totally', u'grid', u'ordered', u'strategic', u'rocket', u'force', u'standby', u'ready', u'hit']\n",
        "[u'south', u'korean', u'target', u'moment', u'notice']\n",
        "[u'see', u'south', u'korean', u'starting', u'wonder', u'aloud', u'whether', u'stay', u'side', u'red', u'line', u'building', u'nuclear', u'bomb']\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def pos_tag_sents(sents):\n",
      "    from nltk.tag import pos_tag\n",
      "    tagged_sents = [pos_tag(sent) for sent in sents]\n",
      "    return tagged_sents"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "tokenized_sents = tokenize_and_normalize_doc(clean_article, filter_stopwords=False, normalize=None)\n",
      "tagged_sents = pos_tag_sents(tokenized_sents)\n",
      "print tagged_sents[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[(u'yes', 'NNS'), (u'its', 'PRP$'), (u'true', 'JJ'), (u'a', 'DT'), (u'crisis', 'NN'), (u'is', 'VBZ'), (u'a', 'DT'), (u'terrible', 'JJ'), (u'thing', 'NN'), (u'to', 'TO'), (u'waste', 'VB')], [(u'but', 'CC'), (u'a', 'DT'), (u'timeout', 'NN'), (u'is', 'VBZ'), (u'also', 'RB'), (u'a', 'DT'), (u'terrible', 'JJ'), (u'thing', 'NN'), (u'to', 'TO'), (u'waste', 'VB'), (u'and', 'CC'), (u'as', 'IN'), (u'i', 'PRP'), (u'look', 'VBP'), (u'at', 'IN'), (u'the', 'DT'), (u'world', 'NN'), (u'today', 'NN'), (u'i', 'PRP'), (u'wonder', 'VBP'), (u'if', 'IN'), (u'thats', 'NNS'), (u'exactly', 'RB'), (u'what', 'WP'), (u'weve', 'VBP'), (u'just', 'RB'), (u'done', 'VBN')], [(u'weve', 'NN'), (u'wasted', 'VBD'), (u'a', 'DT'), (u'five-year', 'JJ'), (u'timeout', 'NN'), (u'from', 'IN'), (u'geopolitics', 'NNS'), (u'and', 'CC'), (u'if', 'IN'), (u'we', 'PRP'), (u'dont', 'VBP'), (u'wake', 'VBP'), (u'up', 'RP'), (u'and', 'CC'), (u'get', 'VB'), (u'our', 'PRP$'), (u'act', 'NN'), (u'together', 'RB'), (u'as', 'IN'), (u'a', 'DT'), (u'country', 'NN'), (u'and', 'CC'), (u'if', 'IN'), (u'the', 'DT'), (u'chinese', 'JJ'), (u'russians', 'NNS'), (u'and', 'CC'), (u'europeans', 'NNS'), (u'dont', 'VBP'), (u'do', 'VBP'), (u'the', 'DT'), (u'same', 'JJ'), (u'were', 'VBD'), (u'all', 'DT'), (u'really', 'RB'), (u'going', 'VBG'), (u'to', 'TO'), (u'regret', 'VB'), (u'it', 'PRP')]]\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def chunk_tagged_sents(tagged_sents):\n",
      "    \n",
      "    from nltk.chunk import regexp\n",
      "    \n",
      "    grammar = r\"\"\"\n",
      "\t\tNP: {<DT|PP\\$>?<JJ>*<NN.*>+} # noun phrase\n",
      "\t\tPP: {<IN><NP>}               # prepositional phrase\n",
      "\t\tVP: {<MD>?<VB.*><NP|PP>}     # verb phrase\n",
      "\t\tCLAUSE: {<NP><VP>}           # full clause\n",
      "\t\"\"\"\n",
      "    chunker = regexp.RegexpParser(grammar, loop=2)\n",
      "    chunked_sents = [chunker.parse(tagged_sent) for tagged_sent in tagged_sents]\n",
      "    \n",
      "    return chunked_sents\n",
      "\n",
      "def get_chunks(chunked_sents, chunk_type='NP'):\n",
      "    \n",
      "    all_chunks = []\n",
      "    for tree in chunked_sents:\n",
      "        chunks = []\n",
      "        raw_chunks = [subtree.leaves() for subtree in tree.subtrees()\n",
      "                      if subtree.node == chunk_type]\n",
      "        for raw_chunk in raw_chunks:\n",
      "            chunk = []\n",
      "            for word_tag in raw_chunk:\n",
      "                chunk.append(word_tag[0])\n",
      "            chunks.append(' '.join(chunk))\n",
      "        all_chunks.append(chunks)\n",
      "    \n",
      "    return all_chunks"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "chunked_sents = chunk_tagged_sents(tagged_sents)\n",
      "chunked_sents[:3]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "[Tree('S', [Tree('NP', [(u'yes', 'NNS')]), (u'its', 'PRP$'), (u'true', 'JJ'), Tree('CLAUSE', [Tree('NP', [(u'a', 'DT'), (u'crisis', 'NN')]), Tree('VP', [(u'is', 'VBZ'), Tree('NP', [(u'a', 'DT'), (u'terrible', 'JJ'), (u'thing', 'NN')])])]), (u'to', 'TO'), (u'waste', 'VB')]),\n",
        " Tree('S', [(u'but', 'CC'), Tree('NP', [(u'a', 'DT'), (u'timeout', 'NN')]), (u'is', 'VBZ'), (u'also', 'RB'), Tree('NP', [(u'a', 'DT'), (u'terrible', 'JJ'), (u'thing', 'NN')]), (u'to', 'TO'), (u'waste', 'VB'), (u'and', 'CC'), (u'as', 'IN'), (u'i', 'PRP'), Tree('VP', [(u'look', 'VBP'), Tree('PP', [(u'at', 'IN'), Tree('NP', [(u'the', 'DT'), (u'world', 'NN'), (u'today', 'NN')])])]), (u'i', 'PRP'), Tree('VP', [(u'wonder', 'VBP'), Tree('PP', [(u'if', 'IN'), Tree('NP', [(u'thats', 'NNS')])])]), (u'exactly', 'RB'), (u'what', 'WP'), (u'weve', 'VBP'), (u'just', 'RB'), (u'done', 'VBN')]),\n",
        " Tree('S', [Tree('CLAUSE', [Tree('NP', [(u'weve', 'NN')]), Tree('VP', [(u'wasted', 'VBD'), Tree('NP', [(u'a', 'DT'), (u'five-year', 'JJ'), (u'timeout', 'NN')])])]), Tree('PP', [(u'from', 'IN'), Tree('NP', [(u'geopolitics', 'NNS')])]), (u'and', 'CC'), (u'if', 'IN'), (u'we', 'PRP'), (u'dont', 'VBP'), (u'wake', 'VBP'), (u'up', 'RP'), (u'and', 'CC'), (u'get', 'VB'), (u'our', 'PRP$'), Tree('NP', [(u'act', 'NN')]), (u'together', 'RB'), Tree('PP', [(u'as', 'IN'), Tree('NP', [(u'a', 'DT'), (u'country', 'NN')])]), (u'and', 'CC'), Tree('PP', [(u'if', 'IN'), Tree('NP', [(u'the', 'DT'), (u'chinese', 'JJ'), (u'russians', 'NNS')])]), (u'and', 'CC'), Tree('NP', [(u'europeans', 'NNS')]), (u'dont', 'VBP'), (u'do', 'VBP'), (u'the', 'DT'), (u'same', 'JJ'), (u'were', 'VBD'), (u'all', 'DT'), (u'really', 'RB'), (u'going', 'VBG'), (u'to', 'TO'), (u'regret', 'VB'), (u'it', 'PRP')])]"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "get_chunks(chunked_sents[0:3], chunk_type='NP')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "pyout",
       "prompt_number": 12,
       "text": [
        "[[u'yes', u'a crisis', u'a terrible thing'],\n",
        " [u'a timeout', u'a terrible thing', u'the world today', u'thats'],\n",
        " [u'weve',\n",
        "  u'a five-year timeout',\n",
        "  u'geopolitics',\n",
        "  u'act',\n",
        "  u'a country',\n",
        "  u'the chinese russians',\n",
        "  u'europeans']]"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}